{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "899bbc0d-fd55-4e41-86ac-67c4964ef4be",
   "metadata": {},
   "source": [
    "# Interannotation Agreement Study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c439f80c-7156-415d-bb7d-061a27e3991e",
   "metadata": {},
   "source": [
    "## 0. Introduction\n",
    "\n",
    "- We have three complete groups of annotation from three annotators from the group, so the annotation distribution of each annotators matters. \n",
    "\n",
    "- Based on the annotations we have, the Cohen's Kappa metric is used in our Interannotation Agreement Study. \n",
    "\n",
    "- We applied the Cohen's Kappa metric separately to the aspects annotation and the overall(aspect+sentiment) annotation and expect to get a higher agreement on average aspects annotation. \n",
    "\n",
    "- We compare the annotation in (annotator) pairs and average them in the end. The agreement scores in pairs tell us which annotators might have a higher agreement with the other two annotators and help us pick the final version of the annotation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1decad-1fc4-433b-9713-03d272edec31",
   "metadata": {},
   "source": [
    "## 1. Experimenting with annotation options (Optional)\n",
    "\n",
    "In order to improve annotation quality and annotation efficiency. We've done two things:\n",
    "\n",
    "1. In order to improve the interannotation agreement/quality, we randomly extracted 10 samples from the data set and discussed together in order to align our understandings of the annotation guideline. We later updated the annotation guideline to reduce ambiguity based on the team discussion.\n",
    "\n",
    "2. To improve the annotation efficiency, we extracted some keywords related to the aspects we should label so we can have an quick prediction about the sentence we will annotate. This is useful especially when the sentence is long and implicit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0b8ca4-806b-4daf-bb8a-3d57174afe63",
   "metadata": {},
   "source": [
    "## 2. Load the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "581e8534-91b1-4f79-bc94-c59d0b343a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in /Users/qichao/opt/miniconda3/lib/python3.9/site-packages (3.0.9)\n",
      "Requirement already satisfied: et-xmlfile in /Users/qichao/opt/miniconda3/lib/python3.9/site-packages (from openpyxl) (1.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/Users/qichao/opt/miniconda3/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a228b8c3-6451-446a-8c64-0314e62aac79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.metrics.agreement import AnnotationTask\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5016a9-e431-4071-a645-801d9e94fd1c",
   "metadata": {},
   "source": [
    "## 3. Importing and Processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5e1c87c-9fd7-4919-b012-b6d58b85dd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "Andrew_annotation = pd.read_excel('raw_manual_annotation/Andrew_climate_change_edit_v3_n521_shuffled.xlsx', index_col=0, header=0)\n",
    "Yuesheng_annotation = pd.read_excel('raw_manual_annotation/Yuesheng_climate_change_edit_v3_n521_shuffled.xlsx', index_col=0, header=0)\n",
    "Jiang_annotation = pd.read_excel('raw_manual_annotation/Jiang_climate_change_edit_v3_n521_shuffled.xlsx', index_col=0, header=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4b8ab19-d75f-4b06-8aed-8a5496dd10f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_columns(df):\n",
    "    '''Add extra columns to indicate if aspects are annotated'''\n",
    "    \n",
    "    df.columns = ['id', 'title', 'focus_sentence', 'aspect_politics_sentiment', 'aspect_humanity_sentiment',\n",
    "       'aspect_science_sentiment', 'aspect_economy_sentiment']\n",
    "    \n",
    "    df[\"aspect_politics\"] = df[\"aspect_politics_sentiment\"] == 'None'\n",
    "    df[\"aspect_humanity\"] = df[\"aspect_humanity_sentiment\"] == 'None'\n",
    "    df[\"aspect_science\"] = df[\"aspect_science_sentiment\"] == 'None'\n",
    "    df[\"aspect_economy\"] = df[\"aspect_economy_sentiment\"] == 'None'\n",
    "    \n",
    "    #convert the bool into string type so we can later make the annotation triple.\n",
    "    mask = df.applymap(type) != bool\n",
    "    d = {True: 'TRUE', False: 'FALSE'}\n",
    "    df = df.where(mask, df.replace(d))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b1df9dd-5169-403a-9004-b78f40a7a04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Andrew_annotation = expand_columns(Andrew_annotation)\n",
    "Yuesheng_annotation = expand_columns(Yuesheng_annotation)\n",
    "Jiang_annotation = expand_columns(Jiang_annotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20501a03-8ada-4e63-a478-39756418a064",
   "metadata": {},
   "source": [
    "## 4. Interannotator agreement study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db58edc3-33ce-4d89-8a19-d89955495b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_triples(annotator, df, col):\n",
    "    '''extract triples from a dataframe based on one column names,\n",
    "        each triple contains (an annotator name, a data id, an annotation of the column)'''\n",
    "    triples = []\n",
    "    for ind, id_ in enumerate(list(df['id'])):\n",
    "        triple = (annotator, id_, col+\"_\"+list(df[col])[ind])\n",
    "        triples.append(triple)\n",
    "    return triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6d1cf8c-ccbc-4c7a-9265-527a9079842b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_kappa_score(annotators, df_1, df_2, cols):\n",
    "    '''calculate the average Kappa agreement score of two annotators(in a list) based on a list of columns'''\n",
    "    kappas = []\n",
    "    for col in cols:\n",
    "        annotator_1_triples = convert_to_triples(annotators[0], df_1, col)\n",
    "        annotator_2_triples = convert_to_triples(annotators[1], df_2, col)\n",
    "        annotation_task = AnnotationTask(annotator_1_triples+annotator_2_triples)\n",
    "        kappa = annotation_task.kappa()\n",
    "        kappas.append(kappa)\n",
    "    avg_kappa = sum(kappas)/len(cols)\n",
    "    return avg_kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e661fff-8b7c-453f-8b0f-fff595915482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The aspect agreement between Andrew and Yuesheng: 0.4998120533330417\n",
      "The aspect agreement between Andrew and Jiang: 0.5220230237330821\n",
      "The aspect agreement between Jiang and Yuesheng: 0.777201476231673\n",
      "The average inter-annotation agreement score based on aspects is: 0.5996788510992656\n"
     ]
    }
   ],
   "source": [
    "aspect_agreement_1 = get_avg_kappa_score([\"Andrew\",\"Yuesheng\"], Andrew_annotation, Yuesheng_annotation, ['aspect_politics', 'aspect_humanity',\n",
    "       'aspect_science', 'aspect_economy'])\n",
    "aspect_agreement_2 = get_avg_kappa_score([\"Andrew\",\"Jiang\"], Andrew_annotation, Jiang_annotation, ['aspect_politics', 'aspect_humanity',\n",
    "       'aspect_science', 'aspect_economy'])\n",
    "aspect_agreement_3 = get_avg_kappa_score([\"Yuesheng\",\"Jiang\"], Yuesheng_annotation, Jiang_annotation, ['aspect_politics', 'aspect_humanity',\n",
    "       'aspect_science', 'aspect_economy'])\n",
    "\n",
    "\n",
    "print('The aspect agreement between Andrew and Yuesheng:', aspect_agreement_1)\n",
    "print('The aspect agreement between Andrew and Jiang:', aspect_agreement_2)\n",
    "print('The aspect agreement between Jiang and Yuesheng:', aspect_agreement_3)\n",
    "print(\"The average inter-annotation agreement score based on aspects is:\", (aspect_agreement_1+aspect_agreement_2+aspect_agreement_3)/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca9fa36-166d-4c9f-b016-fd654e82324c",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01a4fe9c-3cd0-433c-b12d-44b170460854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall agreement between Andrew and Yuesheng: 0.3962174797118952\n",
      "The overall agreement between Andrew and Jiang: 0.4383249504905105\n",
      "The overall agreement between Yuesheng and Jiang: 0.7004710021298375\n",
      "The average inter-annotation agreement score is: 0.5116711441107477\n"
     ]
    }
   ],
   "source": [
    "overall_agreement_1 = get_avg_kappa_score([\"Andrew\",\"Yuesheng\"], Andrew_annotation, Yuesheng_annotation, ['aspect_politics_sentiment', 'aspect_humanity_sentiment',\n",
    "       'aspect_science_sentiment', 'aspect_economy_sentiment'])\n",
    "overall_agreement_2 = get_avg_kappa_score([\"Andrew\",\"Jiang\"], Andrew_annotation, Jiang_annotation, ['aspect_politics_sentiment', 'aspect_humanity_sentiment',\n",
    "       'aspect_science_sentiment', 'aspect_economy_sentiment'])\n",
    "overall_agreement_3 = get_avg_kappa_score([\"Yuesheng\",\"Jiang\"], Yuesheng_annotation, Jiang_annotation, ['aspect_politics_sentiment', 'aspect_humanity_sentiment',\n",
    "       'aspect_science_sentiment', 'aspect_economy_sentiment'])\n",
    "\n",
    "print('The overall agreement between Andrew and Yuesheng:', overall_agreement_1)\n",
    "print('The overall agreement between Andrew and Jiang:', overall_agreement_2)\n",
    "print('The overall agreement between Yuesheng and Jiang:', overall_agreement_3)\n",
    "print(\"The average inter-annotation agreement score is:\", (overall_agreement_1+overall_agreement_2+overall_agreement_3)/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbbc3c6-61b0-4f05-af63-2f12ddf27cb3",
   "metadata": {},
   "source": [
    "## 5. Extract annotation by voting\n",
    "\n",
    "\n",
    "In order to get data with higher quality annotation, we extract the annotation by voting. That is, we keep the annotations that more than 1 annotators agree on a data point, otherwise the data is annotated as None. The extracted annotation will be presented in our interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7084e220-e16a-452c-8c7c-cc85bdb981cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#re-load the original data\n",
    "Andrew_annotation = pd.read_excel('raw_manual_annotation/Andrew_climate_change_edit_v3_n521_shuffled.xlsx', index_col=0, header=0)\n",
    "Yuesheng_annotation = pd.read_excel('raw_manual_annotation/Yuesheng_climate_change_edit_v3_n521_shuffled.xlsx', index_col=0, header=0)\n",
    "Jiang_annotation = pd.read_excel('raw_manual_annotation/Jiang_climate_change_edit_v3_n521_shuffled.xlsx', index_col=0, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8df95530-877a-4411-8a2e-6dbd3da6b931",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_annotation(df_1, df_2, df_3, cols):\n",
    "    '''extract the annotation by voting. \n",
    "    That is, it keeps the annotations that more than 1 annotators agree on a data point, otherwise the data is annotated as None.\n",
    "    The dataframe with extracted annotations is returned'''\n",
    "    extracted_annotation = defaultdict(list)\n",
    "    \n",
    "    for col in cols:\n",
    "        for ind in range(len((df_1[col]))):\n",
    "\n",
    "            annotation_1 = list(df_1[col])[ind]\n",
    "            annotation_2 = list(df_2[col])[ind]\n",
    "            annotation_3 = list(df_3[col])[ind]\n",
    "\n",
    "            anno, count = Counter([annotation_1,annotation_2,annotation_3]).most_common(1)[0]\n",
    "\n",
    "            if count >1:\n",
    "                extracted_annotation[col].append(anno)\n",
    "            else:\n",
    "                extracted_annotation[col].append('None')\n",
    "    return pd.DataFrame(extracted_annotation)\n",
    "extracted_df = extract_annotation(Andrew_annotation, Yuesheng_annotation, Jiang_annotation, ['aspect_politics', 'aspect_humanity',\n",
    "       'aspect_science', 'aspect_economy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a5611b66-4889-4732-94bc-8afb0a38882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_annotation = extracted_annotation.copy()\n",
    "for col in extracted_df.columns:\n",
    "    extracted_annotation[col] = list(extracted_df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c7f540da-329b-4dfa-9569-3919197f5631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The extracted annotation is written to Excel File successfully.\n"
     ]
    }
   ],
   "source": [
    "extracted_annotation.to_excel(\"extracted_annotation.xlsx\")\n",
    "print('The extracted annotation is written to Excel File successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998499c0-3977-4401-b6b3-2152a9935213",
   "metadata": {},
   "source": [
    "## 6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38b540e-ddf8-4b2a-8e5f-2fdb2dad21fd",
   "metadata": {},
   "source": [
    "1. As expected, the average interannotation agreement score based on aspects is higher than the overall score, because the aspect is easier to identify.\n",
    "\n",
    "2. By inspecting the scores between two annotators, we find that Jiang has a higher agreement score with other two annotators. The annotators Jiang and Yuesheng reach the highest agreement score either on aspects or overall annotation. This is probably because the two annotators share a similar cultural background and thus show closest perception of a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6f39f2-e09b-4b16-8e28-71017b4ba407",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
